{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_DNN .ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 02. DNN with 텐서플로우케라스\n","---\n","\n","본 내용은 [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155) 을 바탕으로 공부하고 정리한 것임을 밝힙니다.\n","\n","**목차**\n",">* 순전파\n","  1. 행렬곱 연산\n","  2. 병렬 연산\n","* 다층 퍼셉트론의 순전파\n","  1. 행렬곱 연산\n","* 손실함수\n","  1. MSE(Mean Squared Error, MSE)\n","  2. 이진 크로스 엔트로피(Binary Cross-Entropy)\n","  3. 카테고리칼 크로스 엔트로피(Categorical Cross-Entropy)\n","  4. 그 외에 다양한 손실 함수들\n","* 옵티마이저\n","  1. 모멘텀(Momentum)\n","  2. 아다그라드(Adagrad)\n","  3. 알엠에스프롭(RMSprop)\n","  4. 아담(Adam)\n","* 역전파\n","\n","텐서플로우 케라스 : https://www.tensorflow.org/?hl=ko"],"metadata":{"id":"9Q1yeunDnbDS"}},{"cell_type":"markdown","source":["---\n","## 순전파(Foward Propagation)\n","---\n","활성화 함수, 은닉층의 수, 각 은닉층의 뉴런 수 등 딥 러닝 모델을 설계하고나면 입력값은 입력층, 은닉층을 지나면서 각 층에서의 가중치와 함께 연산되며 출력층으로 향합니다. 그리고 출력층에서 모든 연산을 마친 예측값이 나오게 됩니다. 이와 같이 입력층에서 출력층 방향으로 예측값의 연산이 진행되는 과정을 순전파라고 합니다.\n","\n","<p align='center'>\n","<img src=https://wikidocs.net/images/page/37406/nn2_final_final.PNG width=400>\n","\n","\n","\n","\n"],"metadata":{"id":"1oZm9Wpql4QZ"}},{"cell_type":"markdown","source":["### **(1) 행렬곱 연산**\n","\n","아래 신경망 그림에서 화살표 각각은 가중치 를 의미하고 있습니다. 3개의 뉴런과 2개의 뉴런 사이에는 총 6개의 화살표가 존재하는데, 이는 위 신경망에서 가중치 의 개수가 6개임을 의미합니다.\n","\n","\n","<p align='center'>\n","<img src=https://wikidocs.net/images/page/150781/nn.PNG width=450>\n","</p>\n","\n","\n","일반적으로 동그란 뉴런과 화살표로 표현하는 인공 신경망의 그림에서는 편향 의 경우에는 편의상 생략되는 경우가 많지만, 인공 신경망 내부적으로는 편향 의 연산 또한 존재합니다. 위 그림에서 뉴런과 화살표로 표현한 인공 신경망의 그림에서는 편향을 표현하지 않았지만, 행렬 연산식에서는  표현하였습니다. \n"],"metadata":{"id":"keV3bfEYmcMc"}},{"cell_type":"markdown","source":["**텐서플로우 를 이용한 구현**"],"metadata":{"id":"MzqveLvV0dks"}},{"cell_type":"code","source":["from tensorflow.keras import models, layers"],"metadata":{"id":"jeyRZ80axX07","executionInfo":{"status":"ok","timestamp":1641433583544,"user_tz":-540,"elapsed":3275,"user":{"displayName":"후루룩국수","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtGyKYb6i5WWFwj7aZ6IMmk6q_JPq4zpMobig8=s64","userId":"06687059043568234927"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["x = layers.Input(shape=(3,))\n","y = layers.Dense(2)(x)\n","\n","model = models.Model(x, y)"],"metadata":{"id":"ol1G6zFpzzQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ik1Od79dmpDD","outputId":"acc7989a-f60b-4e65-94bb-b15762069283"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 3)]               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 8         \n","                                                                 \n","=================================================================\n","Total params: 8\n","Trainable params: 8\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["<p align='center'>\n","<img src=https://wikidocs.net/images/page/150781/matrix_multiplication.PNG width=450>\n","</p>\n","\n","\n","* 학습 파라미터 확인\n","  * `get_weights()`"],"metadata":{"id":"YoDO6Y0L0CMe"}},{"cell_type":"code","source":["W, B = model.get_weights()\n","print(W, W.shape)\n","print(B, B.shape)"],"metadata":{"id":"j3n0jlJCz1S-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"15070882-466d-45b0-dbc2-64c8af26e1ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1.0068762  -1.0460497 ]\n"," [-1.0541595  -0.74557555]\n"," [-0.77009255  0.78064084]] (3, 2)\n","[0. 0.] (2,)\n"]}]},{"cell_type":"markdown","source":["### **(2) 병렬연산**\n","\n","인공 신경망을 행렬곱으로 구현할 때의 흥미로운 점은 행렬곱을 사용하면 병렬 연산도 가능하다는 점입니다. 인공 신경망이 4개의 샘플을 동시에 처리해본다고 가정해봅시다. 4개의 샘플을 하나의 행렬 로 정의하고 인공 신경망의 순전파를 행렬곱으로 표현하면 다음과 같습니다.\n","\n","<p align='center'>\n","<img src=https://wikidocs.net/images/page/150781/parallel_nn.PNG width=500>\n","</p>\n","\n","여기서 혼동하지 말아야 할 것은 인공 신경망의 4개의 샘플을 동시에 처리하고 있지만, 여기서 학습가능한 매개변수의 수는 여전히 8개라는 점입니다. 이렇게 인공 신경망이 다수의 샘플을 동시에 처리하는 것을 우리는 **'배치 연산'** 이라고 부릅니다."],"metadata":{"id":"2B95PNoM0O9a"}},{"cell_type":"markdown","source":["---\n","## 다층 퍼셉트론의 순전파\n","---\n","\n","<p align='center'>\n","<img src=https://wikidocs.net/images/page/24987/neuralnetwork_final.PNG width=320>"],"metadata":{"id":"iF1Pr3EaoAF7"}},{"cell_type":"markdown","source":["### **(1) 행렬곱 연산**\n","\n","위 그림에 해당하는 인공신경망의 데이터와 학습 파라미터의 크기를 구하세요\n"],"metadata":{"id":"SANpL92z1-bd"}},{"cell_type":"markdown","source":["* $x, z_1, z_2, y$\n"],"metadata":{"id":"pBcmDE8ZybwG"}},{"cell_type":"code","source":["'''\n","x  : 4\n","z1 : 8\n","z2 : 8\n","y  : 3 \n","'''"],"metadata":{"id":"lc2W98-amfyP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* $W_1, W_2, W_3$ \n"],"metadata":{"id":"1f3wtqGoyz4n"}},{"cell_type":"code","source":["'''\n","W1 : (4,8)\n","W2 : (8,8)\n","W3 : (8,3)\n","'''"],"metadata":{"id":"YGL2joIdyz4o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* $b_1, b_2, b_3$ \n"],"metadata":{"id":"-C7u23xKy0Xf"}},{"cell_type":"code","source":["'''\n","b1 = (8)\n","b2 = (8)\n","b3 = (3)\n","'''"],"metadata":{"id":"2hfl_Pg0y0Xg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**텐서플로우 를 이용한 구현**"],"metadata":{"id":"B5AjRa732e7p"}},{"cell_type":"code","source":["x = layers.Input(shape=4)\n","z1 = layers.Dense(8)(x)\n","z2 = layers.Dense(8)(z1)\n","y = layers.Dense(3)(z2)\n","\n","model = models.Model(x, y)"],"metadata":{"id":"CgMWjNgu2e7y","executionInfo":{"status":"ok","timestamp":1641433624750,"user_tz":-540,"elapsed":390,"user":{"displayName":"후루룩국수","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtGyKYb6i5WWFwj7aZ6IMmk6q_JPq4zpMobig8=s64","userId":"06687059043568234927"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"4JybRkoN2e7z","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b41d9478-23b7-4097-a0cf-9ad185942139","executionInfo":{"status":"ok","timestamp":1641433625028,"user_tz":-540,"elapsed":6,"user":{"displayName":"후루룩국수","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtGyKYb6i5WWFwj7aZ6IMmk6q_JPq4zpMobig8=s64","userId":"06687059043568234927"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 4)]               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 8)                 40        \n","                                                                 \n"," dense_4 (Dense)             (None, 8)                 72        \n","                                                                 \n"," dense_5 (Dense)             (None, 3)                 27        \n","                                                                 \n","=================================================================\n","Total params: 139\n","Trainable params: 139\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["\n","* 학습 파라미터 확인\n","  * `get_weights()`"],"metadata":{"id":"HHVkDTml2e70"}},{"cell_type":"code","source":["# w1, b1, w2, b2, w3, b3\n","for param in model.get_weights():\n","  print(param.shape)"],"metadata":{"id":"gpLHTBB72e71","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b3221f49-13c4-4624-f751-000ba5ec9977","executionInfo":{"status":"ok","timestamp":1641433625029,"user_tz":-540,"elapsed":5,"user":{"displayName":"후루룩국수","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtGyKYb6i5WWFwj7aZ6IMmk6q_JPq4zpMobig8=s64","userId":"06687059043568234927"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 8)\n","(8,)\n","(8, 8)\n","(8,)\n","(8, 3)\n","(3,)\n"]}]},{"cell_type":"markdown","source":["---\n","## 손실함수 (Loss function)\n","---\n","\n","**손실함수란?**\n","\n","> 예측 값과 실제 값의 차이를 계산하는 함수(식)\n","\n","\n","<p align='center'>\n","<img src=https://wikidocs.net/images/page/36033/%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98.PNG\n"," width=450>"],"metadata":{"id":"SyZBvZqH3X9q"}},{"cell_type":"markdown","source":["### **(1)  MSE(Mean Squared Error, MSE)**\n","---\n","평균 제곱 오차는 선형 회귀를 학습할 때 배웠던 손실 함수입니다. 연속형 변수를 예측할 때 사용됩니다."],"metadata":{"id":"MrLhtyhe3X9s"}},{"cell_type":"markdown","source":["**텐서플로우 를 이용한 구현**\n","\n","* `mse` model.compile(loss='mse', optimizer='sgd')\n","* `tf.keras.losses.MeanSquaredError()`\n","(import tensorflow as tf, \n","from tensorflow.keras import losses)"],"metadata":{"id":"r89j9Fh03s_T"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer='sgd')"],"metadata":{"id":"h0u5kYS03s_U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **(2) 이진 크로스 엔트로피(Binary Cross-Entropy)**\n","---\n","\n","이항 교차 엔트로피라고도 부르는 손실 함수입니다. 출력층에서 시그모이드 함수를 사용하는 이진 분류 (Binary Classification)의 경우 `binary_crossentropy`를 사용합니다.\n","\n"],"metadata":{"id":"wRkJZHMD4KcT"}},{"cell_type":"markdown","source":["**텐서플로우 를 이용한 구현**\n","\n","* `binary_crossentropy`\n","* `tf.keras.losses.BinaryCrossentropy()`"],"metadata":{"id":"FZvSA-zH3vzr"}},{"cell_type":"code","source":["model.compile(loss='binary_crossentropy', optimizer='sgd')"],"metadata":{"id":"W1eULm_-4KcW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **(3) 카테고리칼 크로스 엔트로피(Categorical Cross-Entropy)**\n","---\n","\n","범주형 교차 엔트로피라고도 부르는 손실 함수입니다. 출력층에서 소프트맥스 함수를 사용하는 다중 클래스 분류(Multi-Class Classification)일 경우 `categorical_crossentropy`를 사용합니다. "],"metadata":{"id":"wmEpzd0N4hZ_"}},{"cell_type":"markdown","source":["**텐서플로우 를 이용한 구현**\n","\n","* `categorical_crossentropy`\n","* `tf.keras.losses.CategoricalCrossentropy()`\n","\n","원-핫 인코딩 과정을 생략하고 정수값 가진 레이블에 대해서 학습을 수행할 때는 아래 함수를 사용할 수 있습니다.\n","\n","* `sparse_categorical_crossentropy`\n","* `tf.keras.losses.SparseCategoricalCrossentropy()`"],"metadata":{"id":"JECbTMf14haC"}},{"cell_type":"code","source":["model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd')"],"metadata":{"id":"MSDgpWKg4haC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **(4) 그 외에 다양한 손실 함수들**\n","---\n","아래의 텐서플로우 공식 문서 링크에서 방금 언급하지 않은 손실 함수 외에도 다양한 손실 함수들을 확인할 수 있습니다.\n","\n","https://www.tensorflow.org/api_docs/python/tf/keras/losses"],"metadata":{"id":"msFTXniw5VWL"}},{"cell_type":"markdown","source":["---\n","## 옵티마이저 (Optimizer)\n","---\n","\n","**옵티마이저란?**\n","\n","> 학습 파라미터를 조절해서 오차를 줄여가도록 하는 함수\n"],"metadata":{"id":"wRzzsjqHFJ8p"}},{"cell_type":"markdown","source":["### **(1) 모멘텀(Momentum)**\n","---\n","전체 함수에 걸쳐 최소값을 **글로벌 미니멈**(Global Minimum) 이라고 하고, 글로벌 미니멈이 아닌 특정 구역에서의 최소값인 **로컬 미니멈**(Local Minimum) 이라고 합니다. \n","\n","\n","<p align='center'>\n","<img src=https://wikidocs.net/images/page/24987/%EB%A1%9C%EC%BB%AC%EB%AF%B8%EB%8B%88%EB%A9%88.PNG\n"," width=250>\n","\n","\n","![]()\n","\n","**모멘텀(Momentum)**은 관성이라는 물리학의 법칙을 응용한 방법입니다. 모멘텀은 **경사 하강법에 관성을 더 해줍니다.** 모멘텀은 경사 하강법에서 계산된 접선의 기울기에 한 시점(step) 전의 접선의 기울기값을 일정한 비율만큼 반영합니다. 이렇게 하면 마치 언덕에서 공이 내려올 때, 중간에 작은 웅덩이에 빠지더라도 관성의 힘으로 넘어서는 효과를 줄 수 있습니다."],"metadata":{"id":"3OA0YlRzFJ80"}},{"cell_type":"markdown","source":["**텐서플로우 를 이용한 구현**\n","\n","* `sgd`\n","* `tf.keras.optimizers.SGD(lr=0.01, momentum=0.9)`\n"],"metadata":{"id":"33gma9QdFJ80"}},{"cell_type":"code","source":["model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(lr=0.01, momentum=0.9))"],"metadata":{"id":"szS_EfD1Fvih","colab":{"base_uri":"https://localhost:8080/"},"outputId":"38a3b6e8-d06f-467c-9ff6-3a1495bc41a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]}]},{"cell_type":"markdown","source":["### **(2) 아다그라드(Adagrad)**\n","---\n","\n","매개변수들은 각자 의미하는 바가 다른데, 모든 매개변수에 동일한 학습률(learning rate)을 적용하는 것은 비효율적입니다. **아다그라드**는 각 매개변수에 **서로 다른 학습률을 적용시킵니다.** 이때 변화가 많은 매개변수는 학습률이 작게 설정되고 변화가 적은 매개변수는 학습률을 높게 설정시킵니다. \n"],"metadata":{"id":"VI0RO5nWF12Y"}},{"cell_type":"markdown","source":["**텐서플로우 를 이용한 구현**\n","\n","* `adagrad`\n","* `tf.keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)\n","`\n"],"metadata":{"id":"2fUd6HiWF12Y"}},{"cell_type":"code","source":["model.compile(loss='sparse_categorical_crossentropy', optimizer='adagrad')"],"metadata":{"id":"_WtLuxTUF12Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **(3) 알엠에스프롭(RMSprop)**\n","---\n","\n","아다그라드는 나중에 가서는 학습률이 지나치게 떨어진다는 단점이 있는데 이를 다른 수식으로 대체하여 이러한 단점을 개선하였습니다.\n"],"metadata":{"id":"nJWabMBtGCn5"}},{"cell_type":"markdown","source":["**텐서플로우 를 이용한 구현**\n","\n","* `rmsprop`\n","* `tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)`\n"],"metadata":{"id":"QtJDoT_NGCn5"}},{"cell_type":"code","source":["model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')"],"metadata":{"id":"XaFex8VlGCn6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **(4) 아담(Adam)**\n","---\n","\n","아담은 알엠에스프롭과 모멘텀 두 가지를 합친 듯한 방법으로, 방향과 학습률 두 가지를 모두 잡기 위한 방법입니다.\n","\n","\n"],"metadata":{"id":"jke9dWFtGOWT"}},{"cell_type":"markdown","source":["**텐서플로우 를 이용한 구현**\n","\n","* `adam`\n","* `tf.keras.optimizers.Adam(lr=0.001)`\n"],"metadata":{"id":"BL8qY9Z-GOWT"}},{"cell_type":"code","source":["model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"],"metadata":{"id":"k6Z1BVWfGOWU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## 역전파 (Back propagation)\n","---\n","\n","옵티마이저가 수행하는 역할이 바로 역전파입니다. 순전파와 역전파가 한번 진행 될때마다 역전파 알고리즘은 모델의 학습 파라미터에 대한 오차의 미분을 계산합니다. 오차를 감소시키기 위해 각 가중치와 편향이 어떻게 바뀌어야 할 지 알 수 있으며 이 계산과정을 역전파라 합니다. \n","\n","\n","<p align='center'>\n","<img src=https://wikidocs.net/images/page/37406/nn4.PNG\n"," width=400>\n"],"metadata":{"id":"OamjH7YrHOq6"}},{"cell_type":"code","source":[""],"metadata":{"id":"qUVcm_qC6QLy"},"execution_count":null,"outputs":[]}]}